{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_yoer6Nged_"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import nltk\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from torch.nn.functional import pad\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaTiqwM_jr82"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, input_channel, out_channel, kernel_sizes, output_dim):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = input_channel, \n",
        "                                              out_channels = out_channel, \n",
        "                                              kernel_size = ks)\n",
        "                                    for ks in kernel_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.linear = nn.Linear(len(kernel_sizes) * out_channel, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, embedded):     \n",
        "        embedded = embedded.permute(0, 2, 1)       \n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        return self.linear(cat)\n",
        "input_channel = 300\n",
        "out_channel = 100\n",
        "kernel_sizes = [3,4,5]\n",
        "output_dim = 5"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmkJzmw8h5Vj"
      },
      "source": [
        "#predict\n",
        "def tokenize_sentence(sentence, word2vec_dict):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    english_stopwords = stopwords.words('english')\n",
        "    sentence = sentence.strip()\n",
        "    tokenized_sentence = [lemmatizer.lemmatize(token.lower()) for token in tokenizer.tokenize(sentence) if token.lower() in word2vec_dict and token.lower() not in english_stopwords]\n",
        "    return tokenized_sentence\n",
        "\n",
        "def load_word2vec_dict(word2vec_dict_paths):\n",
        "  word2vec_dict = []\n",
        "  for path in word2vec_dict_paths:\n",
        "    word2vec = torch.load(path)\n",
        "    word2vec_dict += list(word2vec.items())\n",
        "  return dict(word2vec_dict)\n",
        "\n",
        "def predict(sentence, model_path = \"./models/xentropy_adam_lr0.0001_wd0.0005_bs128\", word2vec_dict_paths = [\"./word2vec/word2vec_dict_{}\".format(i+1) for i in range(10)],max_seq_length = 29):\n",
        "  word2vec_dict = load_word2vec_dict(word2vec_dict_paths)\n",
        "  tokenized_sentence = tokenize_sentence(sentence,word2vec_dict)\n",
        "  embedding = np.array([word2vec_dict[word] for word in tokenized_sentence])\n",
        "\n",
        "  temp = torch.load(model_path,map_location=torch.device('cpu'))  \n",
        "  model = Network(input_channel, out_channel, kernel_sizes, output_dim)\n",
        "  model.load_state_dict(temp['model_state_dict'])\n",
        "  model.eval()\n",
        "  \n",
        "  embedding = np.expand_dims(embedding,axis=0)\n",
        "  embedding = pad(torch.FloatTensor(embedding), (0, 0, 0, max_seq_length - len(embedding)))\n",
        "  outputs = model(embedding)\n",
        "  \n",
        "  _, predicted = torch.max(outputs.data, 1)\n",
        "  return outputs.data, predicted.item() + 1, embedding\n",
        "\n",
        "# Example: input: sentence  output: model_outputs, predicted_rating, sentence_embedding\n",
        "# predict(\"what a wonderful movie! I love it!\")\n",
        "# predict(\"what a terrible movie! I hate it!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}