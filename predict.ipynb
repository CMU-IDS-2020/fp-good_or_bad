{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_yoer6Nged_"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import nltk\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from torch.nn.functional import pad\n",
        "import zipfile\n",
        "import os\n",
        "from os import listdir\n",
        "from zipfile import ZipFile\n",
        "from os.path import isfile, join\n",
        "from urllib.request import urlopen\n",
        "import pickle\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaTiqwM_jr82"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, input_channel, out_channel, kernel_sizes, output_dim):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = input_channel, \n",
        "                                              out_channels = out_channel, \n",
        "                                              kernel_size = ks)\n",
        "                                    for ks in kernel_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.linear = nn.Linear(len(kernel_sizes) * out_channel, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, embedded):     \n",
        "        embedded = embedded.permute(0, 2, 1)       \n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        return self.linear(cat)\n",
        "input_channel = 300\n",
        "out_channel = 100\n",
        "kernel_sizes = [3,4,5]\n",
        "output_dim = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m57o8XB-giM"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
        "# !unzip *.zip\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew3HwYMTGN3S"
      },
      "source": [
        "# read GloVe\n",
        "# citation: https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
        "# def loadGloveModel(File):\n",
        "#     print(\"Loading Glove Model\")\n",
        "#     with open(File, 'r', encoding='utf-8') as f:\n",
        "#         gloveModel = {}\n",
        "#         for line in f:\n",
        "#             splitLines = line.split()\n",
        "#             word = splitLines[0]\n",
        "#             wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
        "#             gloveModel[word] = wordEmbedding\n",
        "#         print(len(gloveModel),\" words loaded!\")\n",
        "#         return gloveModel\n",
        "# word2vec_dict = loadGloveModel(\"./glove.42B.300d.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmkJzmw8h5Vj"
      },
      "source": [
        "#predict\n",
        "def tokenize_sentence(sentence, word2vec_dict):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    english_stopwords = stopwords.words('english')\n",
        "    sentence = sentence.strip()\n",
        "    tokenized_sentence = [lemmatizer.lemmatize(token.lower()) for token in tokenizer.tokenize(sentence) if token.lower() in word2vec_dict and token.lower() not in english_stopwords]\n",
        "    return tokenized_sentence\n",
        "\n",
        "def load_word2vec_dict(word2vec_urls,word2vec_dir):\n",
        "  word2vec_dict = []\n",
        "  for i in range(len(word2vec_urls)):\n",
        "    url = word2vec_urls[i]\n",
        "    file_path = join(word2vec_dir,\"word2vec_dict{}.pt\".format(i))\n",
        "    torch.hub.download_url_to_file(url,file_path)\n",
        "    word2vec = pickle.load(open(file_path, \"rb\" ))\n",
        "    word2vec = list(word2vec.items())\n",
        "    word2vec_dict += word2vec\n",
        "  \n",
        "  return dict(word2vec_dict)\n",
        "      \n",
        "def predict(sentence, model_url = 'https://github.com/CMU-IDS-2020/fp-good_or_bad/raw/main/models/xentropy_adam_lr0.0001_wd0.0005_bs128.pt', word2vec_urls = ['https://github.com/CMU-IDS-2020/fp-good_or_bad/raw/main/word2vec/100d/word2vec_100d_{}.pt'.format(i+1) for i in range(5)],max_seq_length = 29,word2vec_dir = \"./\"):\n",
        "  word2vec_dict = load_word2vec_dict(word2vec_urls,word2vec_dir)\n",
        "  tokenized_sentence = tokenize_sentence(sentence,word2vec_dict)\n",
        "  embedding = np.array([word2vec_dict[word] for word in tokenized_sentence])\n",
        "\n",
        "  model = Network(input_channel, out_channel, kernel_sizes, output_dim)\n",
        "  model.load_state_dict(torch.hub.load_state_dict_from_url(model_url, progress=False))\n",
        "  model.eval()\n",
        "  \n",
        "  embedding = np.expand_dims(embedding,axis=0)\n",
        "  embedding = pad(torch.FloatTensor(embedding), (0, 0, 0, max_seq_length - len(embedding)))\n",
        "  outputs = model(embedding)\n",
        "  \n",
        "  _, predicted = torch.max(outputs.data, 1)\n",
        "  softmax = F.softmax(outputs.data, dim=1)\n",
        "  return softmax, predicted.item() + 1, embedding\n",
        "\n",
        "# Example: input: sentence  output: model_outputs, predicted_rating, sentence_embedding\n",
        "# predict(\"what a wonderful movie! I love it!\")\n",
        "# predict(\"what a terrible movie! I hate it!\")\n",
        "\n",
        "def get_model_url(dataset,learning_rate,batch_size,weight_decay, optimizer):\n",
        "  dataset_map = {'Amazon products' : \"amazon_products\", 'Movie reviews':'movie_reviews', 'Yelp restaurants':\"yelp_restaurants\"}\n",
        "  optimizer_map = {'ADAM':\"adam\",'Stocastic gradient descent with momentum':\"sgdmomentm\"}\n",
        "  dataset = dataset_map[dataset]\n",
        "  optimizer = optimizer_map[optimizer]\n",
        "  if weight_decay == \"5e-4\":\n",
        "    weight_decay == \"0.0005\"\n",
        "  url = \"https://github.com/CMU-IDS-2020/fp-good_or_bad/raw/main/models/\" + dataset + \"/model_state_dict/\"\n",
        "  model_name = \"xentropy_{}_lr{}_wd{}_bs{}.pt\".format(optimizer,learning_rate,weight_decay,batch_size)\n",
        "  url = url + model_name\n",
        "\n",
        "  if dataset == 'movie_reviews':\n",
        "    max_len = 29\n",
        "  elif dataset == \"yelp_restaurants\":\n",
        "    max_len = 245\n",
        "  else:\n",
        "    max_len = 721\n",
        "  return url, max_len\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiI6KEMG5_jV"
      },
      "source": [
        "# word2vec_dict = torch.load(\"/content/drive/MyDrive/good_or_bad/word2vec/word2vec_dict_1\")\n",
        "# word2vec_list = list(word2vec_dict.items())\n",
        "# num = 4\n",
        "# per_cnt = len(word2vec_list) // num\n",
        "# os.chdir('/content/drive/MyDrive/good_or_bad/word2vec')\n",
        "# for i in range(num):\n",
        "#   if i == num-1:\n",
        "#     vec = dict(word2vec_list[i*per_cnt:])\n",
        "#   else:\n",
        "#     vec = dict(word2vec_list[i*per_cnt:(i+1)*per_cnt])\n",
        "#   file_name = \"word2vec_dict{}\".format(i+1)\n",
        "#   zip_file_name = \"word2vec_dict{}.zip\".format(i+1)\n",
        "#   torch.save(vec,file_name)\n",
        "#   ZipFile(zip_file_name, 'w').write(file_name)\n",
        "\n",
        "# os.chdir('/content')\n",
        "\n",
        "\n",
        "# word2vec = torch.load(\"./drive/MyDrive/good_or_bad/word2vec_dict5\")\n",
        "# pickle.dump(word2vec,open( \"./drive/MyDrive/good_or_bad/word2vec/word2vec_dict5.pt\", \"wb\" ))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}