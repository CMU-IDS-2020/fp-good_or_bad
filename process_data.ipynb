{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read GloVe\n",
    "# citation: https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "def loadGloveModel(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    with open(File, 'r', encoding='utf-8') as f:\n",
    "        gloveModel = {}\n",
    "        for line in f:\n",
    "            splitLines = line.split()\n",
    "            word = splitLines[0]\n",
    "            wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "            gloveModel[word] = wordEmbedding\n",
    "        print(len(gloveModel),\" words loaded!\")\n",
    "        return gloveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read movie reviews data\n",
    "# tokenize -> lowercase -> remove stopwords -> lemmatize\n",
    "def get_movie_reviews_data(path, data_type = \"train\"):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    english_stopwords = stopwords.words('english')\n",
    "    if data_type == \"train\":\n",
    "        with open(path) as f:\n",
    "            lines = list(f.readlines())[1:]\n",
    "            sentences = [line.split('\\t')[2] for line in lines]\n",
    "            labels = [int(line.split('\\t')[3]) for line in lines]\n",
    "            tokenized_sentences = [[lemmatizer.lemmatize(token.lower()) for token in tokenizer.tokenize(sentence) if token.lower() in word2vec_dict and token.lower() not in english_stopwords] for sentence in sentences]\n",
    "            zipped = [(x, y) for x, y in zip(tokenized_sentences, labels) if x != []]\n",
    "            tokenized_sentences = [x for x, y in zipped]\n",
    "            labels = [y for x, y in zipped]\n",
    "            return tokenized_sentences, labels\n",
    "    elif data_type == \"test\":\n",
    "        with open(path) as f:\n",
    "            lines = list(f.readlines())[1:]\n",
    "            sentences = [line.split('\\t')[2] for line in lines]\n",
    "            tokenized_sentences = [[lemmatizer.lemmatize(token.lower()) for token in tokenizer.tokenize(sentence) if token.lower() in word2vec_dict and token.lower() not in english_stopwords] for sentence in sentences]\n",
    "            tokenized_sentences = [x for x in tokenized_sentences if x != []]\n",
    "            return tokenized_sentences, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings\n",
    "def get_embeddings(tokenized_sentences, word2vec_dict):\n",
    "    return [np.array([word2vec_dict[word] for word in x]) for x in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Dataset class\n",
    "class MovieReviewsData(Dataset):\n",
    "    def __init__(self, X, Y = None):\n",
    "        self.maxlen = max(len(x) for x in X)\n",
    "        self.X = [pad(torch.FloatTensor(x), (0, 0, 0, self.maxlen - len(x))) for x in X]\n",
    "        if Y is not None:\n",
    "            self.Y = torch.LongTensor(Y)\n",
    "        else:\n",
    "            self.Y = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.Y is not None:\n",
    "            return self.X[idx], self.Y[idx]\n",
    "        else:\n",
    "            return self.X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "1917495  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "word2vec_dict = loadGloveModel('glove.42B.300d/glove.42B.300d.txt')\n",
    "train_tokenized_sentences, train_Y = get_movie_reviews_data(\"sentiment-analysis-on-movie-reviews/train.tsv\", \"train\")\n",
    "test_tokenized_sentences, _ = get_movie_reviews_data(\"sentiment-analysis-on-movie-reviews/test.tsv\", \"test\")\n",
    "train_X = get_embeddings(train_tokenized_sentences, word2vec_dict)\n",
    "test_X = get_embeddings(test_tokenized_sentences, word2vec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_X.npy', train_X)\n",
    "np.save('train_Y.npy', train_Y)\n",
    "np.save('test_X.npy', test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieReviewsData(train_X, train_Y)\n",
    "test_dataset = MovieReviewsData(test_X)\n",
    "train_loader = DataLoader(train_dataset, shuffle = True, batch_size = BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, shuffle = False, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
